# -*- coding: utf-8 -*-
"""DBMS Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pxl_ZG9Ft0ii6m07KkBQbQNuDSSdAMKM

# Required Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,  roc_auc_score
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.multiclass import OneVsRestClassifier
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
import pickle
import joblib

"""# Data Acquisition"""

# loading csv data to panda dataframe
heart_data = pd.read_csv('/content/heart_disease_uci.csv')

# printing first 5 values of the dataset
heart_data.head()

# @title chol

from matplotlib import pyplot as plt
heart_data['chol'].plot(kind='line', figsize=(8, 4), title='chol')
plt.gca().spines[['top', 'right']].set_visible(False)

# @title trestbps

from matplotlib import pyplot as plt
heart_data['trestbps'].plot(kind='line', figsize=(8, 4), title='trestbps')
plt.gca().spines[['top', 'right']].set_visible(False)

# @title Cholesterol vs. Resting Blood Pressure by Sex

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
for sex in heart_data['sex'].unique():
    subset = heart_data[heart_data['sex'] == sex]
    plt.scatter(subset['trestbps'], subset['chol'], label=sex, alpha=0.7)
plt.title('Cholesterol vs. Resting Blood Pressure by Sex')
plt.xlabel('Resting Blood Pressure (trestbps)')
plt.ylabel('Cholesterol (chol)')
_ = plt.legend()

# @title chol

from matplotlib import pyplot as plt
heart_data['chol'].plot(kind='hist', bins=20, title='chol')
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title trestbps

from matplotlib import pyplot as plt
heart_data['trestbps'].plot(kind='hist', bins=20, title='trestbps')
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title sex

from matplotlib import pyplot as plt
heart_data['sex'].plot(kind='hist', bins=20, title='sex')
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title age

from matplotlib import pyplot as plt
heart_data['age'].plot(kind='hist', bins=20, title='age')
plt.gca().spines[['top', 'right',]].set_visible(False)

# printing last 5 rows of the dataset
heart_data.tail()

# number of rows and coloums in the dataset
heart_data.shape

# getting some info about the data
heart_data.info()

# basic descriptive
heart_data.describe().transpose()

"""# Filtering Dataset"""

# checking for missing values
heart_data.isnull().sum()/len(heart_data)*100

"""Data Visualization"""

# how to visualize relative of the missingness coloums
import missingno as msno
msno.bar(heart_data)
plt.show()

msno.matrix(heart_data)
plt.show()

msno.heatmap(heart_data)
plt.show()

df_numeric = heart_data.select_dtypes(include=['number'])

corr = df_numeric.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, fmt='.2f', square=True)
plt.title('Correlation Matrix')
plt.show()

"""Handling Missing Values"""

# Drop Columns with High Missing Values and Unwanted Columns
heart_data.drop(['id','dataset','ca', 'thal'], axis=1, inplace=True)

# mpute Missing Values for Other Columns
heart_data['trestbps'] = heart_data['trestbps'].fillna(heart_data['trestbps'].mean())
heart_data['chol'] = heart_data['chol'].fillna(heart_data['chol'].mean())
heart_data['fbs'] = heart_data['fbs'].fillna(heart_data['fbs'].mean())
heart_data['thalach'] = heart_data['thalach'].fillna(heart_data['thalach'].mean())
heart_data['exang'] = heart_data['exang'].fillna(heart_data['exang'].mean())
heart_data['oldpeak'] = heart_data['oldpeak'].fillna(heart_data['oldpeak'].mean())

# Handle Missing Values in Categorical Columns
# For the slope column (33.59% missing), fill missing values with the mode.
heart_data['slope'] = heart_data['slope'].fillna(heart_data['slope'].mode()[0])

# Normalize or Standardize Numerical Columns
scaler = StandardScaler()
heart_data[['trestbps', 'chol', 'thalach']] = scaler.fit_transform(heart_data[['trestbps', 'chol', 'thalach']])

# Convert categorical variables into numerical values using one-hot encoding
heart_data = pd.get_dummies(heart_data, columns=['cp', 'restecg', 'slope'])

# Seeing normalized data
heart_data.boxplot(column=['trestbps', 'chol', 'thalach'])
plt.show()

"""Checking New Revised Filtered Data"""

# number of rows and coloums in the dataset
heart_data.shape

# checking for missing values
heart_data.isnull().sum()/len(heart_data)*100

# getting some info about the data
heart_data.info()

# printing first 5 values of the dataset
heart_data.head()

"""#Generating Machine Learning Model

IMBALANCE IN TARGET VARIABLE
"""

# checking the distribution of Target Variable
heart_data['target'].value_counts(normalize = True)

"""*   0: "Healthy: No signs of heart disease"
*   1: "Early Stage: Potential heart issues, no significant symptoms"
*   2: "Mild Symptoms: Minor heart disease indicators present"
*   3: "Advanced Warning: Significant heart disease risk"
*   4: "Severe: High risk of congestive heart failure"
"""

# distribution of target variable
heart_data['target'].value_counts().plot(kind = 'bar')
plt.show()

# Convert object and bool columns to numeric
for col in heart_data.select_dtypes(include=['object', 'bool']).columns:
  heart_data[col] = heart_data[col].astype('category').cat.codes

# Prepare the data
X = heart_data.drop('target', axis=1)
y = heart_data['target']

# dimensions of matrices
X.shape , y.shape

# Feature Engineering Example
heart_data['age_trestbps_interaction'] = heart_data['age'] * heart_data['trestbps']

# Original feature names before PolynomialFeatures
original_feature_names = X.columns.tolist()

heart_data.info()

# Graphical representation of the correlation of the variables with target
c = ['red', 'green', 'yellow', 'purple', 'blue', 'orange', 'cyan']
heart_data.corrwith(heart_data['target']).plot.bar(figsize=(20, 10), title='Correlation with Target', fontsize=15, rot=45, grid=True, color=c)

# Check data types
print(X.dtypes)
print('\n')
print(y.dtypes)

# Ensure data is numeric
X = X.apply(pd.to_numeric, errors='coerce').fillna(X.mean())
y = pd.to_numeric(y, errors='coerce').fillna(y.mode()[0])

# Add Polynomial Features (Try removing this once)
poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)
X_poly = poly.fit_transform(X)

# Ensure data is scaled
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_poly) # use X_poly if using polynomial features

# save scaler and poly
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(poly, 'poly.pkl')

"""Handle class imbalance using SMOTE (Synthetic Minority Over-sampling Technique)"""

# loading the smote module
from imblearn.over_sampling import SMOTE

# Create Test and Train and SMOTE the X Matrices
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_scaled, y)

# dimensions of matrices
X_res.shape , y_res.shape

# distribution of target variable
y_res.value_counts().plot(kind = 'bar')
plt.show()

# distribution of categories of target variable
y_res.value_counts(normalize = True)

"""# Train and Test"""

# Split the resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.2, random_state = 42, stratify=y_res)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""Logistic Regression"""

logreg_model = LogisticRegression(max_iter=2000, solver='lbfgs')
ovr = OneVsRestClassifier(logreg_model)

# Training the LogisticRegression model with Training data
ovr.fit(X_train, y_train)

# Evaluate model
y_pred_logreg = ovr.predict(X_test)

print("Logistic Regression Model Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_logreg))
print("Classification Report:\n", classification_report(y_test, y_pred_logreg))

# Hyperparameter tuning for Logistic Regression
# Define the parameter grid
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],  # L1 = Lasso, L2 = Ridge
    'solver': ['liblinear']  # 'liblinear' supports both L1 and L2 regularization
}

grid_search = GridSearchCV(LogisticRegression(max_iter=2000), param_grid, cv=5, verbose=2, n_jobs=-1)
grid_search.fit(X_train, y_train)

best_logreg_model = grid_search.best_estimator_

# Evaluate the best Logistic Regression model
y_pred_logreg = best_logreg_model.predict(X_test)

print("Improved Logistic Regression Model Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_logreg))
print("Classification Report:\n", classification_report(y_test, y_pred_logreg))

# Confusion Matrix
import seaborn as sns
cm_log = confusion_matrix(y_test, y_pred_logreg)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_log, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

import numpy as np
import pandas as pd
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assuming y_test and y_pred_logreg are available
# Binarize the output labels for multiclass classification
y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3, 4])
n_classes = y_test_binarized.shape[1]

# Assuming y_pred_logreg is the predicted probabilities
y_pred_prob = best_logreg_model.predict_proba(X_test)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot all ROC curves
plt.figure(figsize=(8, 6))
colors = ['aqua', 'darkorange', 'cornflowerblue', 'darkred', 'purple']
for i, color in enumerate(colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f"ROC curve of class {i} (area = {roc_auc[i]:.2f})")

plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Logistic Regression")
plt.legend(loc="lower right")
plt.show()

"""Random Forest"""

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Training the Random Forest model with Training data
rf_model.fit(X_train, y_train)

# Evaluate model
y_pred_rf = rf_model.predict(X_test)

print("Random Forest Model Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Classification Report:\n", classification_report(y_test, y_pred_rf))

# Hyperparameter tuning for Random Forest
# Define the parameter grid
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Set up the randomized search with 100 different combinations
random_search_rf = RandomizedSearchCV(estimator=rf_model, param_distributions=param_grid_rf, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)
random_search_rf.fit(X_train, y_train)

# Get the best estimator
best_rf_model = random_search_rf.best_estimator_

# Evaluate the improved model
y_pred_rf = best_rf_model.predict(X_test)

print("Improved Random Forest Model Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Classification Report:\n", classification_report(y_test, y_pred_rf))

# Confusion Matrix
import seaborn as sns
cm_log = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_log, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Random Forest")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Support Vector Machine (SVM)"""

svm_model = SVC(probability=True)

# Training the SVM model with Training data
svm_model.fit(X_train, y_train)

# Evaluate model
y_pred_svm = svm_model.predict(X_test)

print("SVM Model Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_svm))
print("Classification Report:\n", classification_report(y_test, y_pred_svm))

# Hyperparameter tuning for SVM
# Define the parameter grid
param_grid_svm = {
    'C': [0.1, 1, 10, 100],
    'gamma': [1, 0.1, 0.01, 0.001],
    'kernel': ['rbf', 'poly', 'sigmoid']
}

# Set up the grid search
grid_search_svm = GridSearchCV(SVC(probability=True), param_grid=param_grid_svm, cv=5, verbose=2, n_jobs=-1)
grid_search_svm.fit(X_train, y_train)

# Get the best estimator
best_svm_model = grid_search_svm.best_estimator_

# Evaluate the improved model
y_pred_svc = best_svm_model.predict(X_test)

print("Improved SVM Model Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_svc))
print("Classification Report:\n", classification_report(y_test, y_pred_svc))

# Confusion Matrix
import seaborn as sns
cm_log = confusion_matrix(y_test, y_pred_svm)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_log, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Support Vector Machine")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""K-Nearest Neighbours"""

knn_model = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto')

# Training the KNN model with Training data
knn_model.fit(X_train, y_train)

# Evaluate model
y_pred_knn = knn_model.predict(X_test)

print("K-Nearest Neighbours Model Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_knn))
print("Classification Report:\n", classification_report(y_test, y_pred_knn))

# Hyperparameter tuning for KNN
# Define the parameter grid for KNN
param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9, 11],
    'weights': ['uniform', 'distance'],
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']
}

grid_search_knn = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid_knn, cv=5, verbose=2, n_jobs=-1)
grid_search_knn.fit(X_train, y_train)

best_knn_model = grid_search_knn.best_estimator_

# Evaluate the improved KNN model
y_pred_knn = best_knn_model.predict(X_test)

print("K-Nearest Neighbors Model Performance after Hyperparameter Tuning:")
print("Accuracy:", accuracy_score(y_test, y_pred_knn))
print("Classification Report:\n", classification_report(y_test, y_pred_knn))

# Confusion Matrix
import seaborn as sns
cm_log = confusion_matrix(y_test, y_pred_knn)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_log, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - K-Nearest Neighbours")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Essembling all models in one"""

# Selecting Best Algorithm
ensemble_model = VotingClassifier(estimators=[
    ('logreg', best_logreg_model),
    ('rf', best_rf_model),
    ('svm', best_svm_model),
    ('knn', best_knn_model),
], voting='soft')

ensemble_model.fit(X_train, y_train)

# Evaluate the ensemble model
y_pred_ensemble = ensemble_model.predict(X_test)

print("Ensemble Model Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_ensemble))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_ensemble))
print("Classification Report:\n", classification_report(y_test, y_pred_ensemble))

# Confusion Matrix
import seaborn as sns
cm_log = confusion_matrix(y_test, y_pred_ensemble)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_log, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Ensemble Model")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# Saving Files of All Models"""

joblib.dump(best_knn_model,"LogisticRegression.pkl")
joblib.dump(best_rf_model,"RandomForest.pkl")
joblib.dump(best_svm_model,"SupportVectorMachine.pkl")
joblib.dump(best_knn_model,"KNearestNeighbors.pkl")
joblib.dump(ensemble_model,"EnsembleModel.pkl")

joblib.dump(scaler, 'scaler.pkl')
joblib.dump(poly, 'poly.pkl')

"""# Rough Codes"""

user_input = {
    'age': float(input("Enter age: ")),
    'sex': float(input("Enter sex (0 for female, 1 for male): ")),
    'trestbps': float(input("Enter resting blood pressure: ")),
    'chol': float(input("Enter serum cholesterol: ")),
    'thalach': float(input("Enter maximum heart rate achieved: ")),
    'fbs': float(input("Enter fasting blood sugar (0 for < 120 mg/dl, 1 for > 120 mg/dl): ")),
    'exang': float(input("Enter exercise induced angina (1 for yes, 0 for no): ")),
    'oldpeak': float(input("Enter ST depression induced by exercise: ")),
    'cp': float(input("Enter chest pain type (asymptomatic, atypical angina, non-anginal, typical angina): ")),
    'restecg': float(input("Enter resting electrocardiographic results (lv hypertrophy, normal, st-t wave abnormality): ")),
    'slope': float(input("Enter the slope of the peak exercise ST segment (downsloping, flat, upsloping): "))
}

# Load your dataset
heart_data = pd.read_csv('/content/heart_disease_uci.csv')

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import joblib

# Drop Columns with High Missing Values and Unwanted Columns
heart_data.drop(['id', 'dataset', 'ca', 'thal'], axis=1, inplace=True)

# Impute Missing Values for Other Columns
heart_data['trestbps'] = heart_data['trestbps'].fillna(heart_data['trestbps'].mean())
heart_data['chol'] = heart_data['chol'].fillna(heart_data['chol'].mean())
heart_data['fbs'] = heart_data['fbs'].fillna(heart_data['fbs'].mean())
heart_data['thalach'] = heart_data['thalach'].fillna(heart_data['thalach'].mean())
heart_data['exang'] = heart_data['exang'].fillna(heart_data['exang'].mean())
heart_data['oldpeak'] = heart_data['oldpeak'].fillna(heart_data['oldpeak'].mean())

# Handle Missing Values in Categorical Columns
heart_data['slope'] = heart_data['slope'].fillna(heart_data['slope'].mode()[0])

# Normalize or Standardize Numerical Columns
scaler = StandardScaler()
heart_data[['trestbps', 'chol', 'thalach']] = scaler.fit_transform(heart_data[['trestbps', 'chol', 'thalach']])

# Convert categorical variables into numerical values using one-hot encoding
heart_data = pd.get_dummies(heart_data, columns=['cp', 'restecg', 'slope'])

# Convert any remaining object and boolean columns to numeric
for col in heart_data.select_dtypes(include=['object', 'bool']).columns:
    heart_data[col] = heart_data[col].astype('category').cat.codes

# Feature Engineering Example
heart_data['age_trestbps_interaction'] = heart_data['age'] * heart_data['trestbps']

# Prepare the data
X = heart_data.drop('target', axis=1)
y = heart_data['target']

# Keep track of feature names
original_feature_names = X.columns.tolist()

# Add Polynomial Features
poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)
X_poly = poly.fit_transform(X)
poly_feature_names = poly.get_feature_names_out(original_feature_names)

# Ensure data is scaled
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_poly)

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_scaled, y)

# Split the resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42, stratify=y_res)

# Convert X_train to DataFrame to maintain columns
X_train_df = pd.DataFrame(X_train, columns=poly_feature_names)

# Fit the model (Example: Logistic Regression)
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Save the model and fitted scaler/poly
joblib.dump(model, 'heart_disease_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(poly, 'poly.pkl')
joblib.dump(X_train_df.columns.tolist(), 'poly_feature_names.pkl')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
import joblib

# 1. Load the Dataset
df = pd.read_csv('heart_disease_uci.csv')

# 2. Preprocessing

# Drop irrelevant columns
df.drop(['id', 'ca', 'thal', 'dataset'], axis=1, inplace=True)

# Handle missing values (numerical: fill with mean, categorical: fill with mode)
for col in df.select_dtypes(include=['float64', 'int64']).columns:
    df[col] = df[col].fillna(df[col].mean())

for col in df.select_dtypes(include=['object', 'bool']).columns:
    df[col] = df[col].fillna(df[col].mode()[0])

# Encode categorical features
categorical_cols = df.select_dtypes(include=['object', 'bool']).columns
encoder = {}
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    encoder[col] = le

# Split data into features and target
X = df.drop('target', axis=1)
y = df['target']

# Scale numerical features
scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# 3. Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 4. Train Model
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train, y_train)

# 5. Evaluate Model
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Save the model and preprocessing objects
joblib.dump(model, 'heart_disease_model_simple.pkl')
joblib.dump(scaler, 'scaler_simple.pkl')
joblib.dump(encoder, 'encoder_simple.pkl')

# 6. Prediction Function
def predict_heart_disease(user_input):
    # Convert input to DataFrame
    input_df = pd.DataFrame(user_input, index=[0])

    # Encode categorical inputs
    for col, le in encoder.items():
        if col in input_df.columns:
            input_df[col] = le.transform(input_df[col])

    # Scale numerical inputs
    input_df = scaler.transform(input_df)

    # Predict
    prediction = model.predict(input_df)[0]
    target_labels = ["Healthy", "Early Stage", "Mild Symptoms", "Advanced Warning", "Severe"]
    return target_labels[int(prediction)]

# 7. Example Usage
user_input = {
    'age': 55,
    'sex': 1,  # 0 for female, 1 for male
    'trestbps': 140,  # resting blood pressure
    'chol': 240,  # serum cholesterol
    'thalach': 150,  # maximum heart rate achieved
    'fbs': 0,  # fasting blood sugar: 0 for < 120 mg/dl, 1 for > 120 mg/dl
    'exang': 1,  # exercise induced angina: 1 for yes, 0 for no
    'oldpeak': 2.3,  # ST depression induced by exercise
    'cp': 'typical angina',  # chest pain type
    'restecg': 'normal',  # resting electrocardiographic results
    'slope': 'flat'  # slope of the peak exercise ST segment
}

# Make prediction
print("Prediction:", predict_heart_disease(user_input))